# `mlutils.py`

A lightweight and extensible PyTorch machine learning framework designed for rapid prototyping and scalable training of deep learning models.

## ğŸš€ Features

- **Distributed Training**: Built-in support for multi-GPU and multi-node training with `torchrun`
- **Mixed Precision**: Automatic mixed precision training with gradient scaling
- **Flexible Architecture**: Easy-to-extend modular design for models, datasets, and callbacks
- **Comprehensive Training**: Automated checkpointing, learning rate scheduling, and statistics tracking
- **Command-line Interface**: YAML configuration with command-line overrides
- **Production Ready**: Gradient clipping, memory monitoring, and robust error handling

## ğŸ“ Project Structure

```
mlutils.py/
â”œâ”€â”€ mlutils/              # Core ML framework
â”‚   â”œâ”€â”€ trainer.py       # Main Trainer class with distributed training support
â”‚   â”œâ”€â”€ callbacks.py     # Callback system for training hooks
â”‚   â”œâ”€â”€ schedule.py      # Learning rate schedulers
â”‚   â””â”€â”€ utils.py         # Utilities (device selection, parameter counting, etc.)
â”œâ”€â”€ project/              # Example project implementation
â”‚   â”œâ”€â”€ models/          # Model implementations
â”‚   â”‚   â””â”€â”€ transformer.py  # Example transformer model
â”‚   â”œâ”€â”€ datasets/        # Dataset implementations
â”‚   â”‚   â”œâ”€â”€ dummy.py     # Example dummy dataset
â”‚   â”‚   â””â”€â”€ utils.py     # Dataset loading utilities
â”‚   â”œâ”€â”€ callbacks.py     # Project-specific callbacks
â”‚   â”œâ”€â”€ utils.py         # Optimizers (AdamW, Lion), normalizers, loss functions
â”‚   â””â”€â”€ __main__.py      # Training script with CLI
â”œâ”€â”€ scripts/             # Installation and utility scripts
â””â”€â”€ pyproject.toml       # Project dependencies
```

## ğŸ› ï¸ Installation

### Prerequisites
- Python >= 3.11
- PyTorch (automatically installed)

### Quick Install

```bash
git clone <repository-url>
cd mlutils.py
pip install -e .
```

### Development Install

For development with additional tools:

```bash
git clone <repository-url>
cd mlutils.py
pip install -e ".[dev]"
```

## ğŸ¯ Quick Start

### Training a Model

Single GPU training:
```bash
python -m project --train true --dataset dummy --exp_name my_experiment --epochs 50
```

Multi-GPU training:
```bash
torchrun --nproc-per-node 2 -m project --train true --dataset dummy --exp_name my_experiment --epochs 50
```

### Evaluating a Model

```bash
python -m project --evaluate true --exp_name my_experiment
```

### Resuming from Checkpoint

```bash
python -m project --restart true --exp_name my_experiment
```

## âš™ï¸ Configuration

The framework uses YAML configuration files with command-line overrides. Key configuration options:

```bash
python -m project --help
```

### Training Parameters
- `--epochs`: Number of training epochs (default: 100)
- `--batch_size`: Batch size (default: 4)
- `--learning_rate`: Learning rate (default: 1e-3)
- `--weight_decay`: Weight decay (default: 0.0)
- `--optimizer`: Optimizer choice (adamw, lion)
- `--mixed_precision`: Enable mixed precision training (default: true)

### Model Parameters
- `--model_type`: Model type (0: Transformer)
- `--channel_dim`: Model hidden dimension (default: 64)
- `--num_blocks`: Number of transformer blocks (default: 4)
- `--num_heads`: Number of attention heads (default: 8)
- `--mlp_ratio`: MLP expansion ratio (default: 4.0)

## ğŸ—ï¸ Core Components

### Trainer Class

The `mlutils.Trainer` class provides:

- **Distributed Training**: Automatic handling of multi-GPU training with DDP
- **Mixed Precision**: Built-in support for FP16 training with gradient scaling
- **Checkpointing**: Automatic model and optimizer state saving/loading
- **Callbacks**: Extensible hook system for custom training logic
- **Statistics**: Training/validation loss tracking and custom metrics

### Example Usage

```python
import mlutils
import project

# Load data
train_data, test_data, metadata = project.load_dataset('dummy', 'data/')

# Create model
model = project.Transformer(
    in_dim=metadata['in_dim'], 
    out_dim=metadata['out_dim'],
    channel_dim=64,
    num_blocks=4,
    num_heads=8
)

# Create trainer
trainer = mlutils.Trainer(
    model=model,
    _data=train_data,
    data_=test_data,
    epochs=100,
    lr=1e-3,
    mixed_precision=True
)

# Train
trainer.train()
```

### Adding Custom Models

Create new models in `project/models/`:

```python
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self, in_dim, out_dim, **kwargs):
        super().__init__()
        self.layers = nn.Linear(in_dim, out_dim)
    
    def forward(self, x):
        return self.layers(x)
```

Register in `project/models/__init__.py`:
```python
from .my_model import MyModel
```

### Adding Custom Datasets

Create new datasets in `project/datasets/`:

```python
import torch.utils.data as data

class MyDataset(data.Dataset):
    def __init__(self, root):
        # Load your data
        pass
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]
```

Register in `project/datasets/utils.py`:
```python
def load_dataset(dataset_name, DATADIR_BASE):
    if dataset_name == 'my_dataset':
        # Load and return train, test, metadata
        pass
```

## ğŸ“Š Results and Monitoring

Training results are automatically saved to `out/<exp_name>/`:

```
out/
â””â”€â”€ my_experiment/
    â”œâ”€â”€ config.yaml          # Experiment configuration
    â”œâ”€â”€ ckpt01/              # Checkpoints (model + optimizer state)
    â”œâ”€â”€ ckpt02/
    â”œâ”€â”€ ...
    â”œâ”€â”€ losses.png           # Training/validation loss plots
    â”œâ”€â”€ learning_rate.png    # Learning rate schedule
    â”œâ”€â”€ grad_norm.png        # Gradient norm tracking
    â””â”€â”€ rel_error.json       # Final evaluation metrics
```

## ğŸ”§ Advanced Features

### Custom Optimizers

The framework includes implementations of:
- **AdamW**: With automatic parameter group separation (decay vs no-decay)
- **Lion**: Memory-efficient optimizer with sign-based updates

### Data Normalizers

Built-in normalizers for data preprocessing:
- `IdentityNormalizer`: No normalization
- `UnitCubeNormalizer`: Scale to [0,1] range
- `UnitGaussianNormalizer`: Zero mean, unit variance

### Callback System

Extensible callback system for training hooks:

```python
class MyCallback(mlutils.Callback):
    def __call__(self, trainer, **kwargs):
        # Custom logic during training
        pass

trainer.add_callback('epoch_end', MyCallback())
```

### Learning Rate Schedules

Supported schedules:
- `OneCycleLR`: One-cycle learning rate policy
- `CosineAnnealingLR`: Cosine annealing
- `CosineAnnealingWarmRestarts`: Cosine annealing with restarts
- `ConstantLR`: Constant learning rate

## ğŸ§ª Example: Dummy Dataset

The included dummy dataset demonstrates the framework:
- **Input**: 2D coordinates (x, y) on a 32Ã—32 grid
- **Output**: sin(Ï€x) Ã— sin(Ï€y) function values
- **Task**: Function approximation with a transformer model

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add your model/dataset/feature following the existing patterns
4. Submit a pull request

## ğŸ“š Citation

If you use this framework in your research, please cite:

```bibtex
@misc{mlutils2025,
  title={mlutils.py: A Lightweight PyTorch ML Framework},
  author={Vedant Puri},
  year={2025},
  url={https://github.com/vpuri3/mlutils.py}
}
```

---

**Note**: This framework is designed to be simple yet powerful. It provides the essential components needed for most ML projects while remaining easy to understand and extend.